<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale = 1.0, maximum-scale=1.0, user-scalable=no" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="keywords"
    content="object rearrangement, habitat, habitat-sim, habitat-api, house cleanup, navigation, deep learning, reinforcement learning, computer vision, neural networks, georgia tech, fair">

  <title>Object Rearrangement in Habitat</title>
  <meta name="description"
    content="Building the rearrangement scenario in Habitat">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.3.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.3.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
    <div class="container">
      <div class="navbar-brand">
        <a class="navbar-item" style="padding-right: 2rem;">
          <b>Object Rearrangement in Habitat</b>
        </a>

        <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navbarMenu">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <hr class="navbar-divider">
      <div id="navbarMenu" class="navbar-menu">
        <div class="navbar-start">
          <a class="navbar-item" href="#task">
            Task Description
          </a>

          <a class="navbar-item" href="#data">
            Data
          </a>

          <a class="navbar-item" href="#embodiement">
            Agent Specification
          </a>

          <a class="navbar-item" href="#evaluation">
            Evaluation
          </a>
          <a class="navbar-item" href="#code">
            Code
          </a>
        </div>
        <div class="navbar-end">
          <div class="navbar-item">
            <a href="//gatech.edu">
              <img style="height:20px;" src="/img/gt.jpg">
            </a>
          </div>
          <div class="navbar-item">
            <a href="//gatech.edu">
              <img style="height:20px;" src="/img/fair.jpg">
            </a>
          </div>
        </div>

      </div>
    </div>

  </nav>
  <section class="section" id="task">
    <div class="container">
      <h1 class="title">
        Task Description
      </h1>
      <figure class="column is-half is-offset-one-quarter image is-16-by-9">
        <img src="img/input_frame.png">
        <div class="has-text-centered"> Figure 1: An example of agent’s egocentric view during anepisode.
          Each episode contains 2-5 YCB objects that needto be rearranged in an indoor environment. The blue circle indicates the cross-hair for the 
          magic pointer grab action.
        </div>
      </figure>
      <br>
      <p>
        In the rearrangement task, the agent is spawned randomly in a
        house and is asked to find a small set of objects scattered
        around the house and place them in their desired final position as efficiently as possible.
      </p>
    </div>
  </section>

  <section class="section" id="data">
    <div class="container">
      <h2 class="title">
        Data
      </h2>
      
      <div class="columns is-desktop">
        <div class="column">
          <p class="has-text-justified">
            <b>Scenes:</b> We use a manually-selected subset of 55 photorealistic scans of indoor environments from the
            Gibson dataset.
            These 55 scenes are uncluttered ‘empty’ apartments/houses, i.e. they contain no or very few furniture as part
            of the scanned mesh. Scanned object meshes are programmatically inserted into these scenes to create episodes.
            This combination of empty houses and inserted objects allows
            for controlled generation of training and testing episodes.
          </p>
          <br>
          <p class="has-text-justified">
            <b>Objects</b>: We use object scans from the YCB Dataset. These objects are small enough that they can pass
            through doors
            and hallways within the house.
          </p>
          <br>
          <p class="has-text-justified">
            <b>Episodes</b>: As illustrated in Figure 1, each episode requires the agent to rearrange
            2-5 objects. At the beginning of each episode, the agent is provided with the goal location and rotation
            of each object via point-coordinates (3D coordinate of the center of mass of the object). The agent is also
            given the spawn location and rotation of the agent, initial object location, rotation, and type in the
            environment.
            Finally, for each object, the episode defines goal location and rotation for each object’s centre-of-mass (COM).
            <!-- Overall, the 55 scenes are split into 35 training scenes, 10 validation scenes, and 10 test scenes. 
            Each training scene has 1000 episodes while validation and test scenes have 100 episodes each. 
            Overall, there are 35000 training, 1000 validation, and 1000 test episodes.  -->
          </p>
          <br>
          <div class="is-size-5 has-text-weight-medium">
            <a class="button" href="https://rearrangement.s3.amazonaws.com/rearrangement_dataset.zip"> Download dataset here </a>
          </div>
        </div>
        <div class="column" style="margin-top: -4rem;">
          <pre>
            <code>
              {  
                'episode_id': 0,  
                'scene_id': 'data/scene_datasets/coda/coda.glb’,  
                'start_position': [-0.15, 0.18, 0.29],  
                'start_rotation': [-0.0, -0.34, -0.0, 0.93]  
                'objects’: [
                  {    
                    'object_id': 0,    
                    'object_template': 'data/test_assets/objects/chair',    
                    'position': [1.77, 0.67, -1.99],    
                    'rotation': [0.0, 0.0, 0.0, 1.0]  
                  }
                ],   
                'goals’: [
                  {    
                    'position': [4.34, 0.67, -5.06],    
                    'rotation': [0.0, 0.0, 0.0, 1.0]   
                  }
                ],
              }
            </code>
          </pre>
        </div>
      </div>
    </div>
  </section>
  <section class="section" id="embodiement">
    <div class="container">
      <h2 class="title">
        Agent Specification
      </h2>
      <p class="has-text-justified">
        <b>Agent:</b>  The agent is a virtual Locobot. The
        simulated agent’s base-radius is 0.61m and the height is
        0.175m which matches the LoCoBot dimensions.
        
      </p>
      <br>
      <p class="has-text-justified">
        <b>Sensors</b>: The
        agent is equipped with an RGB-D camera placed at the
        height of 1.5m from the center of the agent’s base and is
        looking down 30 degrees. The sensor has a resolution of 256x256 pixels and a 90 degree field of view.
        To mimic the depth camera’s limitations, we clip simulated
        depth sensing to 10m. The agent is also equipped with a
        GPS+Compass sensor, providing agent location (x, y, z)
        and heading (azimuth angle) in an episodic coordinate system defined by agent’s spawn location (origin) and heading
        (0).
      </p>
      <br>
      <div class="columns">
        <div class="column is-one-third is-offset-one-third">
          <figure class="image is-16-by-9">
            <img src="img/grab_release.png">
            <div class="has-text-justified"> Figure 2: Grab Release Action uses magic pointer abstraction to pick nearby
              objects.
              Any object under a fixed crosshair in the agent’s viewport can be picked by the agent if it is within a
              certain distance threshold.
            </div>
          </figure>
        </div>
      </div>
      <div>
        <p class="has-text-justified">
          <b>Actions</b>: The action space for the rearrangement
          task consists of navigation and interactive actions.
        <ol>
          <li><b>Move Forward:</b> The agent moves forward by 0.25m</li>
          <li><b>Turn Right:</b> The agent turns right by 10 degrees</li>
          <li><b>Turn Left:</b> The agent turns right by 10 degrees</li>
          <li><b>Grab:</b> Grabs an object using the magic pointer abstraction discussed earlier to pick nearby objects
            that are visible in the
            agent’s field of view.</li>
          <li><b>Release:</b> Realeases an object that the agent is holding.</li>
        </ol>
        </p>
      </div>
    </div>
  </section>
  <section class="section" id="evaluation">
    <div class="container">
      <h2 class="title">
        Evaluation
      </h2>
      <p>
        In the object rearrangement scenario, task progress is measured by how close the object is placed with respect to the goal pose. We measure the following metrics: 
        <ol class="is-large">
          <li>
            <b>Average distance to target (\(d_{T}\))</b> measures the distance between the object's current location and the desired goal location. 
          </li>
          <li>
            <b>Episode Success (\(S\))</b> measures whether the episode is successful or not. 
            An episode is considered successful (\(S=1\)) if the all objects stops within 0.50 meters of their respective goal locations, 
            otherwise the episode is marked as failed (\(S=0\)).
          </li>
          <li>
            <b>Object Placement Success \(PS\)</b> which measures the percentage of objects 
            placed successfully according to the success criteria for each episode.
          </li>
          <li>
            <b>Success Weighted by Path Length (\(SPL\))</b> intuitively captures how closely the agent followed the optimal path and successfully completed the episode.. 
            Given the length of the optimal-path trajectory \(l\) and the length of an agent's path \(l_a\) for an episode. 
            SPL is defined as \(\frac{l}{\text{max}(l_a, l)}\). SPL intuitively captures how closely the agent followed the optimal path and successfully completed the episode.
          </li>
        </ol>
        
        
      </p>
    </div>
  </section>
  <section class="section">
    <div class="container">
      <h2 class="title">
        Code
      </h2>
      <p>
        <b> <a href="https://github.com/facebookresearch/habitat-lab/pull/470">Code in Habitat Lab. </a></b>
      </p>
      <p>
        <b>Tutorial: </b>You can watch how to build an interactive task in Habitat Lab by following the tutorial here: 
      </p>
      <p>
        <a href="https://youtu.be/KbbfD7RlJLM?list=PLGywud_-HlCORC0c4uj97oppQrGiB6JNy">Youtube Video </a>,         
        <a href="https://colab.research.google.com/github/facebookresearch/habitat-lab/blob/master/examples/tutorials/colabs/Habitat_Interactive_Tasks.ipynb">Colab Notebook </a>
      </p>
    </div>
  </section>
</body>

<script type="text/javascript" src="/js/jquery.js"></script>
<script>
  $(document).ready(function () {

    // Check for click events on the navbar burger icon
    $(".navbar-burger").click(function () {

      // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
      $(".navbar-burger").toggleClass("is-active");
      $(".navbar-menu").toggleClass("is-active");

    });
  });
</script>

</html>